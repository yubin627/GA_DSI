{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../tokenized_data/X_train.csv',index_col = 0)\n",
    "X_test = pd.read_csv('../tokenized_data/X_test.csv',index_col = 0)\n",
    "y_train = pd.read_csv('../tokenized_data/y_train.csv',index_col = 0)\n",
    "y_test = pd.read_csv('../tokenized_data/y_test.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make use of `GridSearchCV` to tune vectorizer parameters for each of the following 6 classifiers firstly using `CountVectorizer`:\n",
    " - Multinomial Naive Bayes\n",
    " - K-Nearest Neighbors\n",
    " - Logistic Regression\n",
    " - Random Forest\n",
    " - AdaBoost (adaptive boost)\n",
    " - Gradient Boost\n",
    " \n",
    " \n",
    "2. Repeat the GridSearches with varying tokenized input from notebook **02_Preprocessing**, namely, original tokens, stemmed tokens and lemmatized tokens\n",
    "\n",
    "3. Repeat the GridSearches on best-performing version of tokens for `TfidfVectorizer`. \n",
    "\n",
    "4. Fine tune the best classifier coupled with one of the better feature extraction techniques (Vectorizers) using `GridSearchCV`, again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gridsearch_results(X_train, X_test, y_train, y_test, steps_list, steps_titles, pipe_params):\n",
    "    # instantiate results DataFrame\n",
    "    grid_results = pd.DataFrame(columns=['model','best_params','train_accuracy','test_accuracy',\n",
    "                                     'tn','fp','fn','tp',\n",
    "                                     'sensitivity/recall','specificity','precision'])\n",
    "    for i in tqdm(range(len(steps_list))):          # time each iteration\n",
    "        # configure pipeline for each classifier\n",
    "        pipe = Pipeline(steps=steps_list[i])        \n",
    "        # grid search using the default parameters of the classifier\n",
    "        grid = GridSearchCV(pipe, pipe_params, cv=5, n_jobs=-1) \n",
    "\n",
    "        model_results = {}\n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        model_results['model'] = steps_titles[i]\n",
    "        model_results['best_params'] = grid.best_params_\n",
    "        model_results['train_accuracy'] = grid.score(X_train, y_train)\n",
    "        model_results['test_accuracy'] = grid.score(X_test, y_test)\n",
    "\n",
    "        # Store confusion matrix results \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, grid.predict(X_test)).ravel() \n",
    "        model_results['tn'] = tn\n",
    "        model_results['fp'] = fp\n",
    "        model_results['fn'] = fn\n",
    "        model_results['tp'] = tp\n",
    "        model_results['sensitivity/recall'] = tp / (tp + fn)\n",
    "        model_results['specificity'] = tn / (tn + fp)\n",
    "        model_results['precision'] = tp / (tp + fp)\n",
    "        \n",
    "        print('Model: ',steps_titles[i])\n",
    "        print('Best Params: ', grid.best_params_)\n",
    "        \n",
    "        grid_results = grid_results.append(model_results, ignore_index=True)\n",
    "        #print(grid_results)\n",
    "    return grid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_list_gr_cv = [\n",
    "    [('cv',CountVectorizer()),('multi_nb',MultinomialNB())],\n",
    "    [('cv',CountVectorizer()),('knn',KNeighborsClassifier())], \n",
    "    [('cv',CountVectorizer()),('logreg',LogisticRegression())],\n",
    "    [('cv',CountVectorizer()),('rf',RandomForestClassifier())],\n",
    "    [('cv',CountVectorizer()),('ada',AdaBoostClassifier())],\n",
    "    [('cv',CountVectorizer()),('gb',GradientBoostingClassifier())]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_titles_list = ['multi_nb','knn','logreg','rf','ada','gb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_cv =  {\"cv__stop_words\":[None, 'english'], \n",
    "                   \"cv__ngram_range\":[(1,1),(1,2)], \n",
    "                   'cv__max_df' : [1.0, 0.90]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use original tokens (without stemming or lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = X_train['post']\n",
    "X_test_raw = X_test['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:17<01:27, 17.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  multi_nb\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 2/6 [00:44<01:21, 20.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  knn\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 3/6 [01:06<01:02, 20.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  logreg\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 4/6 [01:29<00:42, 21.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  rf\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|████████▎ | 5/6 [02:17<00:29, 29.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  ada\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6/6 [05:03<00:00, 70.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  gb\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "grid_results_cv = gridsearch_results(X_train_raw, X_test_raw, y_train, y_test, steps_list_gr_cv, steps_titles_list, pipe_params_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>sensitivity/recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multi_nb</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.968304</td>\n",
       "      <td>0.933387</td>\n",
       "      <td>286</td>\n",
       "      <td>52</td>\n",
       "      <td>30</td>\n",
       "      <td>863</td>\n",
       "      <td>0.966405</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.943169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.995124</td>\n",
       "      <td>0.920390</td>\n",
       "      <td>279</td>\n",
       "      <td>59</td>\n",
       "      <td>39</td>\n",
       "      <td>854</td>\n",
       "      <td>0.956327</td>\n",
       "      <td>0.825444</td>\n",
       "      <td>0.935378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gb</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.935214</td>\n",
       "      <td>0.913079</td>\n",
       "      <td>250</td>\n",
       "      <td>88</td>\n",
       "      <td>19</td>\n",
       "      <td>874</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.739645</td>\n",
       "      <td>0.908524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logreg</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...</td>\n",
       "      <td>0.999652</td>\n",
       "      <td>0.911454</td>\n",
       "      <td>243</td>\n",
       "      <td>95</td>\n",
       "      <td>14</td>\n",
       "      <td>879</td>\n",
       "      <td>0.984323</td>\n",
       "      <td>0.718935</td>\n",
       "      <td>0.902464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ada</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...</td>\n",
       "      <td>0.925810</td>\n",
       "      <td>0.896832</td>\n",
       "      <td>239</td>\n",
       "      <td>99</td>\n",
       "      <td>28</td>\n",
       "      <td>865</td>\n",
       "      <td>0.968645</td>\n",
       "      <td>0.707101</td>\n",
       "      <td>0.897303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knn</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.848091</td>\n",
       "      <td>179</td>\n",
       "      <td>159</td>\n",
       "      <td>28</td>\n",
       "      <td>865</td>\n",
       "      <td>0.968645</td>\n",
       "      <td>0.529586</td>\n",
       "      <td>0.844727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model                                        best_params  \\\n",
       "0  multi_nb  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "3        rf  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "5        gb  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "2    logreg  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...   \n",
       "4       ada  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...   \n",
       "1       knn  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "\n",
       "   train_accuracy  test_accuracy   tn   fp  fn   tp  sensitivity/recall  \\\n",
       "0        0.968304       0.933387  286   52  30  863            0.966405   \n",
       "3        0.995124       0.920390  279   59  39  854            0.956327   \n",
       "5        0.935214       0.913079  250   88  19  874            0.978723   \n",
       "2        0.999652       0.911454  243   95  14  879            0.984323   \n",
       "4        0.925810       0.896832  239   99  28  865            0.968645   \n",
       "1        0.898990       0.848091  179  159  28  865            0.968645   \n",
       "\n",
       "   specificity  precision  \n",
       "0     0.846154   0.943169  \n",
       "3     0.825444   0.935378  \n",
       "5     0.739645   0.908524  \n",
       "2     0.718935   0.902464  \n",
       "4     0.707101   0.897303  \n",
       "1     0.529586   0.844727  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results_cv.sort_values('test_accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes looks quite good already with original tokens. Let's re-run the classifiers on the other two versions of tokens and see if stemming/lemmatizing is going to help at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use stemming tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_st = X_train['post_st']\n",
    "X_test_st = X_test['post_st']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Please skip this chunk of gridsearch to save time if you are testing the code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 1/6 [00:19<01:36, 19.22s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  multi_nb\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|███▎      | 2/6 [00:43<01:22, 20.66s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  knn\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 3/6 [01:04<01:02, 20.98s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  logreg\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|██████▋   | 4/6 [01:24<00:41, 20.69s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  rf\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 83%|████████▎ | 5/6 [02:05<00:26, 26.77s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  ada\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 6/6 [04:25<00:00, 60.67s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  gb\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "grid_results_cv = gridsearch_results(X_train_st, X_test_st, y_train, y_test, steps_list_gr_cv, steps_titles_list, pipe_params_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>sensitivity/recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multi_nb</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.963079</td>\n",
       "      <td>0.929326</td>\n",
       "      <td>288</td>\n",
       "      <td>50</td>\n",
       "      <td>37</td>\n",
       "      <td>856</td>\n",
       "      <td>0.958567</td>\n",
       "      <td>0.852071</td>\n",
       "      <td>0.944812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.996169</td>\n",
       "      <td>0.915516</td>\n",
       "      <td>270</td>\n",
       "      <td>68</td>\n",
       "      <td>36</td>\n",
       "      <td>857</td>\n",
       "      <td>0.959686</td>\n",
       "      <td>0.798817</td>\n",
       "      <td>0.926486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logreg</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...</td>\n",
       "      <td>0.999303</td>\n",
       "      <td>0.909017</td>\n",
       "      <td>245</td>\n",
       "      <td>93</td>\n",
       "      <td>19</td>\n",
       "      <td>874</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.724852</td>\n",
       "      <td>0.903826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gb</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.939046</td>\n",
       "      <td>0.908205</td>\n",
       "      <td>248</td>\n",
       "      <td>90</td>\n",
       "      <td>23</td>\n",
       "      <td>870</td>\n",
       "      <td>0.974244</td>\n",
       "      <td>0.733728</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ada</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...</td>\n",
       "      <td>0.930338</td>\n",
       "      <td>0.907392</td>\n",
       "      <td>247</td>\n",
       "      <td>91</td>\n",
       "      <td>23</td>\n",
       "      <td>870</td>\n",
       "      <td>0.974244</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.905307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knn</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.895507</td>\n",
       "      <td>0.839968</td>\n",
       "      <td>187</td>\n",
       "      <td>151</td>\n",
       "      <td>46</td>\n",
       "      <td>847</td>\n",
       "      <td>0.948488</td>\n",
       "      <td>0.553254</td>\n",
       "      <td>0.848697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model                                        best_params  \\\n",
       "0  multi_nb  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "3        rf  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "2    logreg  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...   \n",
       "5        gb  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "4       ada  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...   \n",
       "1       knn  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "\n",
       "   train_accuracy  test_accuracy   tn   fp  fn   tp  sensitivity/recall  \\\n",
       "0        0.963079       0.929326  288   50  37  856            0.958567   \n",
       "3        0.996169       0.915516  270   68  36  857            0.959686   \n",
       "2        0.999303       0.909017  245   93  19  874            0.978723   \n",
       "5        0.939046       0.908205  248   90  23  870            0.974244   \n",
       "4        0.930338       0.907392  247   91  23  870            0.974244   \n",
       "1        0.895507       0.839968  187  151  46  847            0.948488   \n",
       "\n",
       "   specificity  precision  \n",
       "0     0.852071   0.944812  \n",
       "3     0.798817   0.926486  \n",
       "2     0.724852   0.903826  \n",
       "5     0.733728   0.906250  \n",
       "4     0.730769   0.905307  \n",
       "1     0.553254   0.848697  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results_cv.sort_values('test_accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like stemming is not really helping! This matches with our initial speculation that for a novel series like Harry Potter / Fantastic Beasts that involve a lot of words coided by the author, stemming/lemmatizing may not be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use lemmatizing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lm = X_train['post_lm']\n",
    "X_test_lm = X_test['post_lm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Please skip this chunk of gridsearch to save time if you are testing the code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 1/6 [00:16<01:22, 16.49s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  multi_nb\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|███▎      | 2/6 [00:44<01:19, 20.00s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  knn\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 3/6 [01:06<01:01, 20.58s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  logreg\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|██████▋   | 4/6 [01:29<00:42, 21.29s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  rf\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 83%|████████▎ | 5/6 [02:10<00:27, 27.18s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  ada\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 6/6 [04:44<00:00, 65.37s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  gb\n",
      "Best Params:  {'cv__max_df': 0.9, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_results_cv = gridsearch_results(X_train_lm, X_test_lm, y_train, y_test, steps_list_gr_cv, steps_titles_list, pipe_params_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>sensitivity/recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multi_nb</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.963079</td>\n",
       "      <td>0.929326</td>\n",
       "      <td>288</td>\n",
       "      <td>50</td>\n",
       "      <td>37</td>\n",
       "      <td>856</td>\n",
       "      <td>0.958567</td>\n",
       "      <td>0.852071</td>\n",
       "      <td>0.944812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.995124</td>\n",
       "      <td>0.913891</td>\n",
       "      <td>274</td>\n",
       "      <td>64</td>\n",
       "      <td>42</td>\n",
       "      <td>851</td>\n",
       "      <td>0.952968</td>\n",
       "      <td>0.810651</td>\n",
       "      <td>0.930055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logreg</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...</td>\n",
       "      <td>0.999303</td>\n",
       "      <td>0.909017</td>\n",
       "      <td>245</td>\n",
       "      <td>93</td>\n",
       "      <td>19</td>\n",
       "      <td>874</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.724852</td>\n",
       "      <td>0.903826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ada</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...</td>\n",
       "      <td>0.930338</td>\n",
       "      <td>0.907392</td>\n",
       "      <td>247</td>\n",
       "      <td>91</td>\n",
       "      <td>23</td>\n",
       "      <td>870</td>\n",
       "      <td>0.974244</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.905307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gb</td>\n",
       "      <td>{'cv__max_df': 0.9, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.935563</td>\n",
       "      <td>0.907392</td>\n",
       "      <td>247</td>\n",
       "      <td>91</td>\n",
       "      <td>23</td>\n",
       "      <td>870</td>\n",
       "      <td>0.974244</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.905307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knn</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.895507</td>\n",
       "      <td>0.839968</td>\n",
       "      <td>187</td>\n",
       "      <td>151</td>\n",
       "      <td>46</td>\n",
       "      <td>847</td>\n",
       "      <td>0.948488</td>\n",
       "      <td>0.553254</td>\n",
       "      <td>0.848697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model                                        best_params  \\\n",
       "0  multi_nb  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "3        rf  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "2    logreg  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...   \n",
       "4       ada  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...   \n",
       "5        gb  {'cv__max_df': 0.9, 'cv__ngram_range': (1, 1),...   \n",
       "1       knn  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "\n",
       "   train_accuracy  test_accuracy   tn   fp  fn   tp  sensitivity/recall  \\\n",
       "0        0.963079       0.929326  288   50  37  856            0.958567   \n",
       "3        0.995124       0.913891  274   64  42  851            0.952968   \n",
       "2        0.999303       0.909017  245   93  19  874            0.978723   \n",
       "4        0.930338       0.907392  247   91  23  870            0.974244   \n",
       "5        0.935563       0.907392  247   91  23  870            0.974244   \n",
       "1        0.895507       0.839968  187  151  46  847            0.948488   \n",
       "\n",
       "   specificity  precision  \n",
       "0     0.852071   0.944812  \n",
       "3     0.810651   0.930055  \n",
       "2     0.724852   0.903826  \n",
       "4     0.730769   0.905307  \n",
       "5     0.730769   0.905307  \n",
       "1     0.553254   0.848697  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results_cv.sort_values('test_accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not very different from the results from stemming tokens. Therefore I will just stick to the original tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_list_gr_tf = [ # list of pipeline steps for each model combo\n",
    "    [('tf',TfidfVectorizer()),('multi_nb',MultinomialNB())],\n",
    "    [('tf',TfidfVectorizer()),('knn',KNeighborsClassifier())], \n",
    "    [('tf',TfidfVectorizer()),('logreg',LogisticRegression())],\n",
    "    [('tf',TfidfVectorizer()),('rf',RandomForestClassifier())],\n",
    "    [('tf',TfidfVectorizer()),('ada',AdaBoostClassifier())],\n",
    "    [('tf',TfidfVectorizer()),('gb',GradientBoostingClassifier())]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_titles_list = ['multi_nb','knn','logreg','rf','ada','gb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_tf = {\"tf__stop_words\":[None, 'english'], \n",
    "                  \"tf__ngram_range\":[(1,1),(1,2)],\n",
    "                  'tf__max_df' : [1.0, 0.90]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Please skip this chunk of gridsearch to save time if you are testing the code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 1/6 [00:21<01:48, 21.69s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  multi_nb\n",
      "Best Params:  {'tf__max_df': 1.0, 'tf__ngram_range': (1, 1), 'tf__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|███▎      | 2/6 [00:51<01:36, 24.22s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  knn\n",
      "Best Params:  {'tf__max_df': 1.0, 'tf__ngram_range': (1, 2), 'tf__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 3/6 [01:15<01:12, 24.07s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  logreg\n",
      "Best Params:  {'tf__max_df': 1.0, 'tf__ngram_range': (1, 1), 'tf__stop_words': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|██████▋   | 4/6 [01:45<00:51, 25.97s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  rf\n",
      "Best Params:  {'tf__max_df': 1.0, 'tf__ngram_range': (1, 1), 'tf__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 83%|████████▎ | 5/6 [02:39<00:34, 34.34s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  ada\n",
      "Best Params:  {'tf__max_df': 1.0, 'tf__ngram_range': (1, 1), 'tf__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 6/6 [06:23<00:00, 91.27s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  gb\n",
      "Best Params:  {'tf__max_df': 1.0, 'tf__ngram_range': (1, 2), 'tf__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_results_cv = gridsearch_results(X_train_raw, X_test_raw, y_train, y_test, steps_list_gr_tf, steps_titles_list, pipe_params_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ada</td>\n",
       "      <td>{'tf__ngram_range': (1, 1), 'tf__stop_words': ...</td>\n",
       "      <td>0.923023</td>\n",
       "      <td>0.894395</td>\n",
       "      <td>233</td>\n",
       "      <td>105</td>\n",
       "      <td>25</td>\n",
       "      <td>868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf</td>\n",
       "      <td>{'tf__ngram_range': (1, 1), 'tf__stop_words': ...</td>\n",
       "      <td>0.995124</td>\n",
       "      <td>0.889521</td>\n",
       "      <td>232</td>\n",
       "      <td>106</td>\n",
       "      <td>30</td>\n",
       "      <td>863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gb</td>\n",
       "      <td>{'tf__ngram_range': (1, 1), 'tf__stop_words': ...</td>\n",
       "      <td>0.921978</td>\n",
       "      <td>0.879773</td>\n",
       "      <td>211</td>\n",
       "      <td>127</td>\n",
       "      <td>21</td>\n",
       "      <td>872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logreg</td>\n",
       "      <td>{'tf__ngram_range': (1, 1), 'tf__stop_words': ...</td>\n",
       "      <td>0.999652</td>\n",
       "      <td>0.858652</td>\n",
       "      <td>210</td>\n",
       "      <td>128</td>\n",
       "      <td>46</td>\n",
       "      <td>847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multi_nb</td>\n",
       "      <td>{'tf__ngram_range': (1, 1), 'tf__stop_words': ...</td>\n",
       "      <td>0.848137</td>\n",
       "      <td>0.810723</td>\n",
       "      <td>107</td>\n",
       "      <td>231</td>\n",
       "      <td>2</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knn</td>\n",
       "      <td>{'tf__ngram_range': (1, 1), 'tf__stop_words': ...</td>\n",
       "      <td>0.726228</td>\n",
       "      <td>0.725426</td>\n",
       "      <td>0</td>\n",
       "      <td>338</td>\n",
       "      <td>0</td>\n",
       "      <td>893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model                                        best_params  \\\n",
       "4       ada  {'tf__ngram_range': (1, 1), 'tf__stop_words': ...   \n",
       "3        rf  {'tf__ngram_range': (1, 1), 'tf__stop_words': ...   \n",
       "5        gb  {'tf__ngram_range': (1, 1), 'tf__stop_words': ...   \n",
       "2    logreg  {'tf__ngram_range': (1, 1), 'tf__stop_words': ...   \n",
       "0  multi_nb  {'tf__ngram_range': (1, 1), 'tf__stop_words': ...   \n",
       "1       knn  {'tf__ngram_range': (1, 1), 'tf__stop_words': ...   \n",
       "\n",
       "   train_accuracy  test_accuracy   tn   fp  fn   tp  \n",
       "4        0.923023       0.894395  233  105  25  868  \n",
       "3        0.995124       0.889521  232  106  30  863  \n",
       "5        0.921978       0.879773  211  127  21  872  \n",
       "2        0.999652       0.858652  210  128  46  847  \n",
       "0        0.848137       0.810723  107  231   2  891  \n",
       "1        0.726228       0.725426    0  338   0  893  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results_tf.sort_values('test_accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of classifiers differ from CountVectorizer. \n",
    "\n",
    "However, it doesn't really improve further on the top-performing MultinomialNB from CountVectorizer.\n",
    "\n",
    "Let's stick to CounterVectorizer + MultinomialNB using original tokens and fine tune this model.\n",
    "\n",
    "| model\t| best_params |\ttrain_accuracy | test_accuracy | tn | fp | fn | tp | sensitivity/recall | specificity | precision\n",
    "|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| multi_nb |  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': None} | 0.968304 | 0.933387 | 286 | 52 | 30 | 863 | 0.966405 | 0.846154 | 0.943169 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check where the predictions go wrong\n",
    "2. Check if imbalance needs to addressed\n",
    "3. Tune classifier parameters and see if it can be further improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('cls', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick the MultinomialNB model with best_params and \n",
    "# understand the gap between actual classification and the prediction\n",
    "cvt = CountVectorizer(stop_words=None, lowercase=True, ngram_range=(1,1))\n",
    "pipeline = Pipeline([\n",
    "    ('vect', cvt),\n",
    "    ('cls', MultinomialNB())\n",
    "]) \n",
    "pipeline.fit(X_train_raw, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_accuracy': 0.9683037269244166, 'test_accuracy': 0.9333874898456539}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results = {}\n",
    "model_results['train_accuracy'] = pipeline.score(X_train_raw, y_train)\n",
    "model_results['test_accuracy'] = pipeline.score(X_test_raw, y_test)\n",
    "model_results \n",
    "# just a confirmation that the model is reproducible - the score should match the GridSearchCV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train, X_test],ignore_index=True)\n",
    "y = pd.concat([y_train, y_test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_hp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_hp\n",
       "0      1\n",
       "1      0\n",
       "2      0\n",
       "3      1\n",
       "4      1"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorrectly classified\n",
    "incorrect_preds = X[(predicted != y['is_hp'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_df = pd.DataFrame({'actual': y['is_hp'][incorrect_preds.index], \n",
    "                             'predicted': predicted[incorrect_preds.index],\n",
    "                             'text': incorrect_preds['all_text']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Opinions On Fantastic Beasts And Where To Find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Looking for quotes for a birthday card  A frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Muggles, no maj, and “can’t spells” What did e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hogwarts Mystery: Jacob's bedroom (might conta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[NO SPOILERS] Snow storm ruined my night and w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     actual  predicted                                               text\n",
       "18        1          0  Opinions On Fantastic Beasts And Where To Find...\n",
       "30        0          1  Looking for quotes for a birthday card  A frie...\n",
       "82        0          1  Muggles, no maj, and “can’t spells” What did e...\n",
       "122       1          0  Hogwarts Mystery: Jacob's bedroom (might conta...\n",
       "186       0          1  [NO SPOILERS] Snow storm ruined my night and w..."
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Is jk rowling Alive? She has completely disapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Why In The World Are Wizards Afraid On Muggles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4067</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Maledictus Question Is there a set age for eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Please share your theories about Grindelwald's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>My cinema is having a screening tonight! Alrea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual  predicted                                               text\n",
       "4038       1          0  Is jk rowling Alive? She has completely disapp...\n",
       "4056       0          1  Why In The World Are Wizards Afraid On Muggles...\n",
       "4067       1          0  Maledictus Question Is there a set age for eve...\n",
       "4096       0          1  Please share your theories about Grindelwald's...\n",
       "4100       0          1  My cinema is having a screening tonight! Alrea..."
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_df.to_csv('../data/incorrect_preds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking a few of the posts, I think it makes sense for the discrepancy between predictions and actual subreddit to happen.\n",
    "There are inherently some link between the Harry Potter series and the Fantastic Beasts series. To me, some of the 'incorrect predictions' actually did a better job than human in classifying the posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, does the imbalance of classes in the dataset matter?\n",
    "\n",
    "I am just going to use simple oversampling on FantasticBeasts dataset to make it match up with HarryPotter sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2082), (1, 2082)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(sorted(Counter(y_train_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled = pd.DataFrame(X_resampled, columns=['post','post_st','post_lm','all_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>post_st</th>\n",
       "      <th>post_lm</th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>they should make a movie about voldemort that ...</td>\n",
       "      <td>they should make a movi about voldemort that e...</td>\n",
       "      <td>they should make a movi about voldemort that e...</td>\n",
       "      <td>They should make a movie about Voldemort that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beauty and the beast get paid homage by j k ro...</td>\n",
       "      <td>beauti and the beast get paid homag by j k row...</td>\n",
       "      <td>beauti and the beast get paid homag by j k row...</td>\n",
       "      <td>Beauty and the Beast get paid Homage by J K Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spoilers now that we know that nagini was in f...</td>\n",
       "      <td>spoiler now that we know that nagini wa in fac...</td>\n",
       "      <td>spoiler now that we know that nagini wa in fac...</td>\n",
       "      <td>[spoilers] Now that we know that nagini was in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>merchandise monday welcome to merchandise mond...</td>\n",
       "      <td>merchandis monday welcom to merchandis monday ...</td>\n",
       "      <td>merchandis monday welcom to merchandis monday ...</td>\n",
       "      <td>Merchandise Monday! Welcome to Merchandise Mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how did witches save themselves when burning a...</td>\n",
       "      <td>how did witch save themselv when burn at the s...</td>\n",
       "      <td>how did witch save themselv when burn at the s...</td>\n",
       "      <td>How did witches save themselves when burning a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  \\\n",
       "0  they should make a movie about voldemort that ...   \n",
       "1  beauty and the beast get paid homage by j k ro...   \n",
       "2  spoilers now that we know that nagini was in f...   \n",
       "3  merchandise monday welcome to merchandise mond...   \n",
       "4  how did witches save themselves when burning a...   \n",
       "\n",
       "                                             post_st  \\\n",
       "0  they should make a movi about voldemort that e...   \n",
       "1  beauti and the beast get paid homag by j k row...   \n",
       "2  spoiler now that we know that nagini wa in fac...   \n",
       "3  merchandis monday welcom to merchandis monday ...   \n",
       "4  how did witch save themselv when burn at the s...   \n",
       "\n",
       "                                             post_lm  \\\n",
       "0  they should make a movi about voldemort that e...   \n",
       "1  beauti and the beast get paid homag by j k row...   \n",
       "2  spoiler now that we know that nagini wa in fac...   \n",
       "3  merchandis monday welcom to merchandis monday ...   \n",
       "4  how did witch save themselv when burn at the s...   \n",
       "\n",
       "                                            all_text  \n",
       "0  They should make a movie about Voldemort that ...  \n",
       "1  Beauty and the Beast get paid Homage by J K Ro...  \n",
       "2  [spoilers] Now that we know that nagini was in...  \n",
       "3  Merchandise Monday! Welcome to Merchandise Mon...  \n",
       "4  How did witches save themselves when burning a...  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_resampled = pd.DataFrame(y_train_resampled, columns=['is_hp']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled_raw = X_train_resampled['post']\n",
    "X_test_raw = X_test['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('cls', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass the resampled training data to te model\n",
    "pipeline = Pipeline([\n",
    "    ('vect', cvt),\n",
    "    ('cls', MultinomialNB())\n",
    "]) \n",
    "pipeline.fit(X_train_resampled_raw, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}\n",
    "model_results['train_accuracy'] = pipeline.score(X_train_resampled_raw, y_train_resampled)\n",
    "model_results['test_accuracy'] = model.score(X_test_raw, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_accuracy': 0.9731027857829011, 'test_accuracy': 0.9317627944760357}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling helped a little bit on the accuracy of training dataset (from 0.968 to 0.973), but doesn't really help further improve the accuracy on test set (from 0.933 to 0.932)...the original model is probably already doing good enough since the impact from imbalance is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV for fine tuning MultinomialNB base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('cv',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('mnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'cv': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'mnb': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'cv__analyzer': 'word',\n",
       " 'cv__binary': False,\n",
       " 'cv__decode_error': 'strict',\n",
       " 'cv__dtype': numpy.int64,\n",
       " 'cv__encoding': 'utf-8',\n",
       " 'cv__input': 'content',\n",
       " 'cv__lowercase': True,\n",
       " 'cv__max_df': 1.0,\n",
       " 'cv__max_features': None,\n",
       " 'cv__min_df': 1,\n",
       " 'cv__ngram_range': (1, 1),\n",
       " 'cv__preprocessor': None,\n",
       " 'cv__stop_words': None,\n",
       " 'cv__strip_accents': None,\n",
       " 'cv__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'cv__tokenizer': None,\n",
       " 'cv__vocabulary': None,\n",
       " 'mnb__alpha': 1.0,\n",
       " 'mnb__class_prior': None,\n",
       " 'mnb__fit_prior': True}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_steps = [('cv',cvt),\n",
    "            ('mnb',MultinomialNB())]\n",
    "\n",
    "# Fine tune parameters for MultinomialNB model\n",
    "mnb_params = {\"mnb__alpha\":np.arange(.05, 2, .05)}\n",
    "\n",
    "pipe = Pipeline(mnb_steps)\n",
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('mnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'mnb__alpha': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  , 1.05, 1.1 ,\n",
       "       1.15, 1.2 , 1.25, 1.3 , 1.35, 1.4 , 1.45, 1.5 , 1.55, 1.6 , 1.65,\n",
       "       1.7 , 1.75, 1.8 , 1.85, 1.9 , 1.95])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GridSearchCV(pipe, mnb_params, cv=5) \n",
    "model.fit(X_train_raw, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_params': {'mnb__alpha': 1.1},\n",
       " 'train_accuracy': 0.9679554162312783,\n",
       " 'test_accuracy': 0.9317627944760357,\n",
       " 'tn': 284,\n",
       " 'fp': 54,\n",
       " 'fn': 30,\n",
       " 'tp': 863,\n",
       " 'sensitivity/recall': 0.9664053751399776,\n",
       " 'specificity': 0.8402366863905325,\n",
       " 'precision': 0.9411123227917121}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_results = {}\n",
    "mnb_results['best_params'] = model.best_params_\n",
    "mnb_results['train_accuracy'] = model.score(X_train_raw, y_train)\n",
    "mnb_results['test_accuracy'] = model.score(X_test_raw, y_test)\n",
    "\n",
    "# Store confusion matrix results \n",
    "tn, fp, fn, tp = confusion_matrix(y_test, model.predict(X_test_raw)).ravel() \n",
    "mnb_results['tn'] = tn\n",
    "mnb_results['fp'] = fp\n",
    "mnb_results['fn'] = fn\n",
    "mnb_results['tp'] = tp\n",
    "mnb_results['sensitivity/recall'] = tp / (tp + fn)\n",
    "mnb_results['specificity'] = tn / (tn + fp)\n",
    "mnb_results['precision'] = tp / (tp + fp)\n",
    "\n",
    "\n",
    "mnb_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really improving...headroom too tight? Let's stick to the MultinomialNB base model which yielded the best accuracy score so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a few iterations, the model that we found most accurate is the MultinomialNB on CountVectorizer with basic tokenizing (no stemming or lemmatizing).\n",
    "\n",
    "Steps are summarized in the read.me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models could be further evaluated / optimized with new posts coming to the subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
