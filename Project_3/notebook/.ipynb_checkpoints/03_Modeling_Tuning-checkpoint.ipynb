{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../tokenized_data/X_train.csv',index_col = 0)\n",
    "X_test = pd.read_csv('../tokenized_data/X_test.csv',index_col = 0)\n",
    "y_train = pd.read_csv('../tokenized_data/y_train.csv',index_col = 0)\n",
    "y_test = pd.read_csv('../tokenized_data/y_test.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make use of `GridSearchCV` to tune vectorizer parameters for each of the following 6 classifiers firstly using `CountVectorizer`:\n",
    " - Multinomial Naive Bayes\n",
    " - K-Nearest Neighbors\n",
    " - Logistic Regression\n",
    " - Random Forest\n",
    " - AdaBoost (adaptive boost)\n",
    " - Gradient Boost\n",
    " \n",
    " \n",
    "2. Repeat the GridSearches with varying tokenized input from notebook **02_Preprocessing**, namely, original tokens, stemmed tokens and lemmatized tokens\n",
    "\n",
    "3. Repeat the GridSearches on best-performing version of tokens for `TfidfVectorizer`. \n",
    "\n",
    "4. Fine tune the best classifier coupled with one of the better feature extraction techniques (Vectorizers) using `GridSearchCV`, again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gridsearch_results(X_train, X_test, y_train, y_test, steps_list, steps_titles, pipe_params):\n",
    "    # instantiate results DataFrame\n",
    "    grid_results = pd.DataFrame(columns=['model','best_params','train_accuracy','test_accuracy',\n",
    "                                     'tn','fp','fn','tp',\n",
    "                                     'sensitivity/recall','specificity','precision'])\n",
    "    for i in tqdm(range(len(steps_list))):          # time each iteration\n",
    "        # configure pipeline for each classifier\n",
    "        pipe = Pipeline(steps=steps_list[i])        \n",
    "        # grid search using the default parameters of the classifier\n",
    "        grid = GridSearchCV(pipe, pipe_params, cv=5, n_jobs=-1) \n",
    "\n",
    "        model_results = {}\n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        model_results['model'] = steps_titles[i]\n",
    "        model_results['best_params'] = grid.best_params_\n",
    "        model_results['train_accuracy'] = grid.score(X_train, y_train)\n",
    "        model_results['test_accuracy'] = grid.score(X_test, y_test)\n",
    "\n",
    "        # Store confusion matrix results \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, grid.predict(X_test)).ravel() \n",
    "        model_results['tn'] = tn\n",
    "        model_results['fp'] = fp\n",
    "        model_results['fn'] = fn\n",
    "        model_results['tp'] = tp\n",
    "        model_results['sensitivity/recall'] = tp / (tp + fn)\n",
    "        model_results['specificity'] = tn / (tn + fp)\n",
    "        model_results['precision'] = tp / (tp + fp)\n",
    "        \n",
    "        print('Model: ',steps_titles[i])\n",
    "        print('Best Params: ', grid.best_params_)\n",
    "        \n",
    "        grid_results = grid_results.append(model_results, ignore_index=True)\n",
    "        #print(grid_results)\n",
    "    return grid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_list_gr_cv = [\n",
    "    [('cv',CountVectorizer()),('multi_nb',MultinomialNB())],\n",
    "    [('cv',CountVectorizer()),('knn',KNeighborsClassifier())], \n",
    "    [('cv',CountVectorizer()),('logreg',LogisticRegression())],\n",
    "    [('cv',CountVectorizer()),('rf',RandomForestClassifier())],\n",
    "    [('cv',CountVectorizer()),('ada',AdaBoostClassifier())],\n",
    "    [('cv',CountVectorizer()),('gb',GradientBoostingClassifier())]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_titles_list = ['multi_nb','knn','logreg','rf','ada','gb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_cv =  {\"cv__stop_words\":['english'], \n",
    "                   \"cv__ngram_range\":[(1,1),(1,2)], \n",
    "                   'cv__max_df' : [1.0, 0.90]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use original tokens (without stemming or lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = X_train['post']\n",
    "X_test_raw = X_test['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:09<00:49,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  multi_nb\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 2/6 [00:21<00:41, 10.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  knn\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:30<00:30, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  logreg\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 4/6 [00:41<00:20, 10.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  rf\n",
      "Best Params:  {'cv__max_df': 0.9, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [01:03<00:13, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  ada\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 6/6 [02:03<00:00, 27.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  gb\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grid_results_cv = gridsearch_results(X_train_raw, X_test_raw, y_train, y_test, steps_list_gr_cv, steps_titles_list, pipe_params_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>sensitivity/recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multi_nb</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.969000</td>\n",
       "      <td>0.929326</td>\n",
       "      <td>284</td>\n",
       "      <td>54</td>\n",
       "      <td>33</td>\n",
       "      <td>860</td>\n",
       "      <td>0.963046</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>0.940919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logreg</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...</td>\n",
       "      <td>0.999652</td>\n",
       "      <td>0.911454</td>\n",
       "      <td>243</td>\n",
       "      <td>95</td>\n",
       "      <td>14</td>\n",
       "      <td>879</td>\n",
       "      <td>0.984323</td>\n",
       "      <td>0.718935</td>\n",
       "      <td>0.902464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf</td>\n",
       "      <td>{'cv__max_df': 0.9, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.996865</td>\n",
       "      <td>0.904955</td>\n",
       "      <td>260</td>\n",
       "      <td>78</td>\n",
       "      <td>39</td>\n",
       "      <td>854</td>\n",
       "      <td>0.956327</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.916309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gb</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.904955</td>\n",
       "      <td>242</td>\n",
       "      <td>96</td>\n",
       "      <td>21</td>\n",
       "      <td>872</td>\n",
       "      <td>0.976484</td>\n",
       "      <td>0.715976</td>\n",
       "      <td>0.900826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ada</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...</td>\n",
       "      <td>0.925810</td>\n",
       "      <td>0.896832</td>\n",
       "      <td>239</td>\n",
       "      <td>99</td>\n",
       "      <td>28</td>\n",
       "      <td>865</td>\n",
       "      <td>0.968645</td>\n",
       "      <td>0.707101</td>\n",
       "      <td>0.897303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knn</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.848091</td>\n",
       "      <td>179</td>\n",
       "      <td>159</td>\n",
       "      <td>28</td>\n",
       "      <td>865</td>\n",
       "      <td>0.968645</td>\n",
       "      <td>0.529586</td>\n",
       "      <td>0.844727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model                                        best_params  \\\n",
       "0  multi_nb  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "2    logreg  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...   \n",
       "3        rf  {'cv__max_df': 0.9, 'cv__ngram_range': (1, 1),...   \n",
       "5        gb  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "4       ada  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...   \n",
       "1       knn  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "\n",
       "   train_accuracy  test_accuracy   tn   fp  fn   tp  sensitivity/recall  \\\n",
       "0        0.969000       0.929326  284   54  33  860            0.963046   \n",
       "2        0.999652       0.911454  243   95  14  879            0.984323   \n",
       "3        0.996865       0.904955  260   78  39  854            0.956327   \n",
       "5        0.931034       0.904955  242   96  21  872            0.976484   \n",
       "4        0.925810       0.896832  239   99  28  865            0.968645   \n",
       "1        0.898990       0.848091  179  159  28  865            0.968645   \n",
       "\n",
       "   specificity  precision  \n",
       "0     0.840237   0.940919  \n",
       "2     0.718935   0.902464  \n",
       "3     0.769231   0.916309  \n",
       "5     0.715976   0.900826  \n",
       "4     0.707101   0.897303  \n",
       "1     0.529586   0.844727  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results_cv.sort_values('test_accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes looks quite good already with original tokens. Let's re-run the classifiers on the other two versions of tokens and see if stemming/lemmatizing is going to help at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use stemming tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_st = X_train['post_st']\n",
    "X_test_st = X_test['post_st']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Please skip this chunk of gridsearch to save time if you are testing the code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:07<00:39,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  multi_nb\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 2/6 [00:22<00:39,  9.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  knn\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:31<00:29,  9.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  logreg\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 4/6 [00:41<00:19,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  rf\n",
      "Best Params:  {'cv__max_df': 0.9, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [01:00<00:12, 12.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  ada\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 6/6 [02:07<00:00, 28.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  gb\n",
      "Best Params:  {'cv__max_df': 0.9, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grid_results_cv = gridsearch_results(X_train_st, X_test_st, y_train, y_test, steps_list_gr_cv, steps_titles_list, pipe_params_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>sensitivity/recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multi_nb</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.963079</td>\n",
       "      <td>0.929326</td>\n",
       "      <td>288</td>\n",
       "      <td>50</td>\n",
       "      <td>37</td>\n",
       "      <td>856</td>\n",
       "      <td>0.958567</td>\n",
       "      <td>0.852071</td>\n",
       "      <td>0.944812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf</td>\n",
       "      <td>{'cv__max_df': 0.9, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.995472</td>\n",
       "      <td>0.913891</td>\n",
       "      <td>271</td>\n",
       "      <td>67</td>\n",
       "      <td>39</td>\n",
       "      <td>854</td>\n",
       "      <td>0.956327</td>\n",
       "      <td>0.801775</td>\n",
       "      <td>0.927253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logreg</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...</td>\n",
       "      <td>0.999303</td>\n",
       "      <td>0.909017</td>\n",
       "      <td>245</td>\n",
       "      <td>93</td>\n",
       "      <td>19</td>\n",
       "      <td>874</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.724852</td>\n",
       "      <td>0.903826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ada</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...</td>\n",
       "      <td>0.930338</td>\n",
       "      <td>0.907392</td>\n",
       "      <td>247</td>\n",
       "      <td>91</td>\n",
       "      <td>23</td>\n",
       "      <td>870</td>\n",
       "      <td>0.974244</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.905307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gb</td>\n",
       "      <td>{'cv__max_df': 0.9, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.935911</td>\n",
       "      <td>0.905768</td>\n",
       "      <td>244</td>\n",
       "      <td>94</td>\n",
       "      <td>22</td>\n",
       "      <td>871</td>\n",
       "      <td>0.975364</td>\n",
       "      <td>0.721893</td>\n",
       "      <td>0.902591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knn</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.895507</td>\n",
       "      <td>0.839968</td>\n",
       "      <td>187</td>\n",
       "      <td>151</td>\n",
       "      <td>46</td>\n",
       "      <td>847</td>\n",
       "      <td>0.948488</td>\n",
       "      <td>0.553254</td>\n",
       "      <td>0.848697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model                                        best_params  \\\n",
       "0  multi_nb  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "3        rf  {'cv__max_df': 0.9, 'cv__ngram_range': (1, 1),...   \n",
       "2    logreg  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...   \n",
       "4       ada  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...   \n",
       "5        gb  {'cv__max_df': 0.9, 'cv__ngram_range': (1, 1),...   \n",
       "1       knn  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "\n",
       "   train_accuracy  test_accuracy   tn   fp  fn   tp  sensitivity/recall  \\\n",
       "0        0.963079       0.929326  288   50  37  856            0.958567   \n",
       "3        0.995472       0.913891  271   67  39  854            0.956327   \n",
       "2        0.999303       0.909017  245   93  19  874            0.978723   \n",
       "4        0.930338       0.907392  247   91  23  870            0.974244   \n",
       "5        0.935911       0.905768  244   94  22  871            0.975364   \n",
       "1        0.895507       0.839968  187  151  46  847            0.948488   \n",
       "\n",
       "   specificity  precision  \n",
       "0     0.852071   0.944812  \n",
       "3     0.801775   0.927253  \n",
       "2     0.724852   0.903826  \n",
       "4     0.730769   0.905307  \n",
       "5     0.721893   0.902591  \n",
       "1     0.553254   0.848697  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results_cv.sort_values('test_accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like stemming is not really helping! This matches with our initial speculation that for a novel series like Harry Potter / Fantastic Beasts that involve a lot of words coided by the author, stemming/lemmatizing may not be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use lemmatizing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lm = X_train['post_lm']\n",
    "X_test_lm = X_test['post_lm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Please skip this chunk of gridsearch to save time if you are testing the code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:07<00:37,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  multi_nb\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 2/6 [00:20<00:36,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  knn\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:30<00:28,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  logreg\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 4/6 [00:39<00:18,  9.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  rf\n",
      "Best Params:  {'cv__max_df': 0.9, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [00:58<00:12, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  ada\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 6/6 [01:53<00:00, 25.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  gb\n",
      "Best Params:  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grid_results_cv = gridsearch_results(X_train_lm, X_test_lm, y_train, y_test, steps_list_gr_cv, steps_titles_list, pipe_params_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>sensitivity/recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multi_nb</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.963079</td>\n",
       "      <td>0.929326</td>\n",
       "      <td>288</td>\n",
       "      <td>50</td>\n",
       "      <td>37</td>\n",
       "      <td>856</td>\n",
       "      <td>0.958567</td>\n",
       "      <td>0.852071</td>\n",
       "      <td>0.944812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf</td>\n",
       "      <td>{'cv__max_df': 0.9, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.997214</td>\n",
       "      <td>0.910642</td>\n",
       "      <td>269</td>\n",
       "      <td>69</td>\n",
       "      <td>41</td>\n",
       "      <td>852</td>\n",
       "      <td>0.954087</td>\n",
       "      <td>0.795858</td>\n",
       "      <td>0.925081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logreg</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...</td>\n",
       "      <td>0.999303</td>\n",
       "      <td>0.909017</td>\n",
       "      <td>245</td>\n",
       "      <td>93</td>\n",
       "      <td>19</td>\n",
       "      <td>874</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.724852</td>\n",
       "      <td>0.903826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ada</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...</td>\n",
       "      <td>0.930338</td>\n",
       "      <td>0.907392</td>\n",
       "      <td>247</td>\n",
       "      <td>91</td>\n",
       "      <td>23</td>\n",
       "      <td>870</td>\n",
       "      <td>0.974244</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.905307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gb</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.937304</td>\n",
       "      <td>0.901706</td>\n",
       "      <td>242</td>\n",
       "      <td>96</td>\n",
       "      <td>25</td>\n",
       "      <td>868</td>\n",
       "      <td>0.972004</td>\n",
       "      <td>0.715976</td>\n",
       "      <td>0.900415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knn</td>\n",
       "      <td>{'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...</td>\n",
       "      <td>0.895507</td>\n",
       "      <td>0.839968</td>\n",
       "      <td>187</td>\n",
       "      <td>151</td>\n",
       "      <td>46</td>\n",
       "      <td>847</td>\n",
       "      <td>0.948488</td>\n",
       "      <td>0.553254</td>\n",
       "      <td>0.848697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model                                        best_params  \\\n",
       "0  multi_nb  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "3        rf  {'cv__max_df': 0.9, 'cv__ngram_range': (1, 1),...   \n",
       "2    logreg  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...   \n",
       "4       ada  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 2),...   \n",
       "5        gb  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "1       knn  {'cv__max_df': 1.0, 'cv__ngram_range': (1, 1),...   \n",
       "\n",
       "   train_accuracy  test_accuracy   tn   fp  fn   tp  sensitivity/recall  \\\n",
       "0        0.963079       0.929326  288   50  37  856            0.958567   \n",
       "3        0.997214       0.910642  269   69  41  852            0.954087   \n",
       "2        0.999303       0.909017  245   93  19  874            0.978723   \n",
       "4        0.930338       0.907392  247   91  23  870            0.974244   \n",
       "5        0.937304       0.901706  242   96  25  868            0.972004   \n",
       "1        0.895507       0.839968  187  151  46  847            0.948488   \n",
       "\n",
       "   specificity  precision  \n",
       "0     0.852071   0.944812  \n",
       "3     0.795858   0.925081  \n",
       "2     0.724852   0.903826  \n",
       "4     0.730769   0.905307  \n",
       "5     0.715976   0.900415  \n",
       "1     0.553254   0.848697  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results_cv.sort_values('test_accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not very different from the results from stemming tokens. Therefore I will just stick to the original tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_list_gr_tf = [ # list of pipeline steps for each model combo\n",
    "    [('tf',TfidfVectorizer()),('multi_nb',MultinomialNB())],\n",
    "    [('tf',TfidfVectorizer()),('knn',KNeighborsClassifier())], \n",
    "    [('tf',TfidfVectorizer()),('logreg',LogisticRegression())],\n",
    "    [('tf',TfidfVectorizer()),('rf',RandomForestClassifier())],\n",
    "    [('tf',TfidfVectorizer()),('ada',AdaBoostClassifier())],\n",
    "    [('tf',TfidfVectorizer()),('gb',GradientBoostingClassifier())]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_titles_list = ['multi_nb','knn','logreg','rf','ada','gb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params_tf = {\"tf__stop_words\":['english'], \n",
    "                  \"tf__ngram_range\":[(1,1),(1,2)],\n",
    "                  'tf__max_df' : [1.0, 0.90]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Please skip this chunk of gridsearch to save time if you are testing the code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:08<00:40,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  multi_nb\n",
      "Best Params:  {'tf__max_df': 1.0, 'tf__ngram_range': (1, 1), 'tf__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 2/6 [00:20<00:37,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  knn\n",
      "Best Params:  {'tf__max_df': 1.0, 'tf__ngram_range': (1, 2), 'tf__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:28<00:26,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  logreg\n",
      "Best Params:  {'tf__max_df': 1.0, 'tf__ngram_range': (1, 1), 'tf__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 4/6 [00:37<00:18,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  rf\n",
      "Best Params:  {'tf__max_df': 0.9, 'tf__ngram_range': (1, 1), 'tf__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [00:57<00:12, 12.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  ada\n",
      "Best Params:  {'tf__max_df': 1.0, 'tf__ngram_range': (1, 1), 'tf__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 6/6 [02:19<00:00, 33.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  gb\n",
      "Best Params:  {'tf__max_df': 0.9, 'tf__ngram_range': (1, 2), 'tf__stop_words': 'english'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grid_results_tf = gridsearch_results(X_train_raw, X_test_raw, y_train, y_test, steps_list_gr_tf, steps_titles_list, pipe_params_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_params</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>sensitivity/recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf</td>\n",
       "      <td>{'tf__max_df': 0.9, 'tf__ngram_range': (1, 1),...</td>\n",
       "      <td>0.997910</td>\n",
       "      <td>0.913891</td>\n",
       "      <td>258</td>\n",
       "      <td>80</td>\n",
       "      <td>26</td>\n",
       "      <td>867</td>\n",
       "      <td>0.970885</td>\n",
       "      <td>0.763314</td>\n",
       "      <td>0.915523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ada</td>\n",
       "      <td>{'tf__max_df': 1.0, 'tf__ngram_range': (1, 1),...</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.913891</td>\n",
       "      <td>256</td>\n",
       "      <td>82</td>\n",
       "      <td>24</td>\n",
       "      <td>869</td>\n",
       "      <td>0.973124</td>\n",
       "      <td>0.757396</td>\n",
       "      <td>0.913775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gb</td>\n",
       "      <td>{'tf__max_df': 0.9, 'tf__ngram_range': (1, 2),...</td>\n",
       "      <td>0.945664</td>\n",
       "      <td>0.913079</td>\n",
       "      <td>254</td>\n",
       "      <td>84</td>\n",
       "      <td>23</td>\n",
       "      <td>870</td>\n",
       "      <td>0.974244</td>\n",
       "      <td>0.751479</td>\n",
       "      <td>0.911950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>knn</td>\n",
       "      <td>{'tf__max_df': 1.0, 'tf__ngram_range': (1, 2),...</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.887084</td>\n",
       "      <td>241</td>\n",
       "      <td>97</td>\n",
       "      <td>42</td>\n",
       "      <td>851</td>\n",
       "      <td>0.952968</td>\n",
       "      <td>0.713018</td>\n",
       "      <td>0.897679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logreg</td>\n",
       "      <td>{'tf__max_df': 1.0, 'tf__ngram_range': (1, 1),...</td>\n",
       "      <td>0.927900</td>\n",
       "      <td>0.885459</td>\n",
       "      <td>211</td>\n",
       "      <td>127</td>\n",
       "      <td>14</td>\n",
       "      <td>879</td>\n",
       "      <td>0.984323</td>\n",
       "      <td>0.624260</td>\n",
       "      <td>0.873757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multi_nb</td>\n",
       "      <td>{'tf__max_df': 1.0, 'tf__ngram_range': (1, 1),...</td>\n",
       "      <td>0.869732</td>\n",
       "      <td>0.827782</td>\n",
       "      <td>128</td>\n",
       "      <td>210</td>\n",
       "      <td>2</td>\n",
       "      <td>891</td>\n",
       "      <td>0.997760</td>\n",
       "      <td>0.378698</td>\n",
       "      <td>0.809264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model                                        best_params  \\\n",
       "3        rf  {'tf__max_df': 0.9, 'tf__ngram_range': (1, 1),...   \n",
       "4       ada  {'tf__max_df': 1.0, 'tf__ngram_range': (1, 1),...   \n",
       "5        gb  {'tf__max_df': 0.9, 'tf__ngram_range': (1, 2),...   \n",
       "1       knn  {'tf__max_df': 1.0, 'tf__ngram_range': (1, 2),...   \n",
       "2    logreg  {'tf__max_df': 1.0, 'tf__ngram_range': (1, 1),...   \n",
       "0  multi_nb  {'tf__max_df': 1.0, 'tf__ngram_range': (1, 1),...   \n",
       "\n",
       "   train_accuracy  test_accuracy   tn   fp  fn   tp  sensitivity/recall  \\\n",
       "3        0.997910       0.913891  258   80  26  867            0.970885   \n",
       "4        0.939394       0.913891  256   82  24  869            0.973124   \n",
       "5        0.945664       0.913079  254   84  23  870            0.974244   \n",
       "1        0.919192       0.887084  241   97  42  851            0.952968   \n",
       "2        0.927900       0.885459  211  127  14  879            0.984323   \n",
       "0        0.869732       0.827782  128  210   2  891            0.997760   \n",
       "\n",
       "   specificity  precision  \n",
       "3     0.763314   0.915523  \n",
       "4     0.757396   0.913775  \n",
       "5     0.751479   0.911950  \n",
       "1     0.713018   0.897679  \n",
       "2     0.624260   0.873757  \n",
       "0     0.378698   0.809264  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_results_tf.sort_values('test_accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of classifiers differ from CountVectorizer. \n",
    "\n",
    "However, it doesn't really improve further on the top-performing MultinomialNB from CountVectorizer.\n",
    "\n",
    "Let's stick to CounterVectorizer + MultinomialNB using original tokens and fine tune this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check where the predictions go wrong\n",
    "2. Check if imbalance needs to addressed\n",
    "3. Tune classifier parameters and see if it can be further improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('cls', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick the MultinomialNB model with best_params and \n",
    "# understand the gap between actual classification and the prediction\n",
    "cvt = CountVectorizer(stop_words='english', lowercase=True, ngram_range=(1,1))\n",
    "pipeline = Pipeline([\n",
    "    ('vect', cvt),\n",
    "    ('cls', MultinomialNB())\n",
    "]) \n",
    "pipeline.fit(X_train_raw, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_accuracy': 0.9690003483106931, 'test_accuracy': 0.9293257514216084}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results = {}\n",
    "model_results['train_accuracy'] = pipeline.score(X_train_raw, y_train)\n",
    "model_results['test_accuracy'] = pipeline.score(X_test_raw, y_test)\n",
    "model_results \n",
    "# just a confirmation that the model is reproducible - the score should match the GridSearchCV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'cls': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = pipeline.named_steps['cls']\n",
    "cv = pipeline.named_steps['vect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers\n",
    "\n",
    "def important_features(vectorizer,classifier,n=30):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words for Fantastic Beasts subreddit\\n\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\\n\")\n",
    "    print(\"Important words for Harry Potter subreddit\\n\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words for Fantastic Beasts subreddit\n",
      "\n",
      "0 1037.0 grindelwald\n",
      "0 716.0 credence\n",
      "0 698.0 dumbledore\n",
      "0 579.0 movie\n",
      "0 567.0 newt\n",
      "0 554.0 just\n",
      "0 521.0 think\n",
      "0 502.0 know\n",
      "0 490.0 like\n",
      "0 315.0 leta\n",
      "0 297.0 beasts\n",
      "0 286.0 fantastic\n",
      "0 282.0 time\n",
      "0 276.0 maybe\n",
      "0 272.0 did\n",
      "0 266.0 albus\n",
      "0 238.0 don\n",
      "0 231.0 theory\n",
      "0 228.0 really\n",
      "0 223.0 love\n",
      "0 214.0 way\n",
      "0 212.0 people\n",
      "0 211.0 crimes\n",
      "0 209.0 queenie\n",
      "0 205.0 family\n",
      "0 198.0 harry\n",
      "0 198.0 does\n",
      "0 194.0 story\n",
      "0 189.0 tina\n",
      "0 187.0 didn\n",
      "-----------------------------------------\n",
      "\n",
      "Important words for Harry Potter subreddit\n",
      "\n",
      "1 2216.0 harry\n",
      "1 1095.0 just\n",
      "1 1027.0 like\n",
      "1 915.0 potter\n",
      "1 660.0 know\n",
      "1 654.0 think\n",
      "1 543.0 time\n",
      "1 524.0 voldemort\n",
      "1 513.0 books\n",
      "1 479.0 ve\n",
      "1 474.0 https\n",
      "1 468.0 hogwarts\n",
      "1 468.0 dumbledore\n",
      "1 459.0 book\n",
      "1 445.0 did\n",
      "1 441.0 don\n",
      "1 434.0 people\n",
      "1 415.0 com\n",
      "1 397.0 really\n",
      "1 390.0 house\n",
      "1 362.0 magic\n",
      "1 343.0 hufflepuff\n",
      "1 336.0 read\n",
      "1 332.0 world\n",
      "1 328.0 ravenclaw\n",
      "1 322.0 does\n",
      "1 311.0 slytherin\n",
      "1 310.0 hermione\n",
      "1 309.0 didn\n",
      "1 305.0 love\n"
     ]
    }
   ],
   "source": [
    "important_features(cv, mnb, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train, X_test],ignore_index=True)\n",
    "y = pd.concat([y_train, y_test],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_hp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_hp\n",
       "0      1\n",
       "1      0\n",
       "2      0\n",
       "3      1\n",
       "4      1"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorrectly classified\n",
    "incorrect_preds = X[(predicted != y['is_hp'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_df = pd.DataFrame({'actual': y['is_hp'][incorrect_preds.index], \n",
    "                             'predicted': predicted[incorrect_preds.index],\n",
    "                             'text': incorrect_preds['all_text']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Opinions On Fantastic Beasts And Where To Find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Looking for quotes for a birthday card  A frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Muggles, no maj, and “can’t spells” What did e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hogwarts Mystery: Jacob's bedroom (might conta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[NO SPOILERS] Snow storm ruined my night and w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     actual  predicted                                               text\n",
       "18        1          0  Opinions On Fantastic Beasts And Where To Find...\n",
       "30        0          1  Looking for quotes for a birthday card  A frie...\n",
       "82        0          1  Muggles, no maj, and “can’t spells” What did e...\n",
       "122       1          0  Hogwarts Mystery: Jacob's bedroom (might conta...\n",
       "186       0          1  [NO SPOILERS] Snow storm ruined my night and w..."
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Is jk rowling Alive? She has completely disapp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Why In The World Are Wizards Afraid On Muggles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4067</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Maledictus Question Is there a set age for eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Please share your theories about Grindelwald's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>My cinema is having a screening tonight! Alrea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual  predicted                                               text\n",
       "4038       1          0  Is jk rowling Alive? She has completely disapp...\n",
       "4056       0          1  Why In The World Are Wizards Afraid On Muggles...\n",
       "4067       1          0  Maledictus Question Is there a set age for eve...\n",
       "4096       0          1  Please share your theories about Grindelwald's...\n",
       "4100       0          1  My cinema is having a screening tonight! Alrea..."
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_df.to_csv('../data/incorrect_preds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking a few of the posts, I think it makes sense for the discrepancy between predictions and actual subreddit to happen.\n",
    "There are inherently some link between the Harry Potter series and the Fantastic Beasts series. To me, some of the 'incorrect predictions' actually did a better job than human in classifying the posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, does the imbalance of classes in the dataset matter?\n",
    "\n",
    "I am just going to use simple oversampling on FantasticBeasts dataset to make it match up with HarryPotter sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2082), (1, 2082)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(sorted(Counter(y_train_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled = pd.DataFrame(X_train_resampled, columns=['post','post_st','post_lm','all_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>post_st</th>\n",
       "      <th>post_lm</th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>they should make a movie about voldemort that ...</td>\n",
       "      <td>they should make a movi about voldemort that e...</td>\n",
       "      <td>they should make a movi about voldemort that e...</td>\n",
       "      <td>They should make a movie about Voldemort that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beauty and the beast get paid homage by j k ro...</td>\n",
       "      <td>beauti and the beast get paid homag by j k row...</td>\n",
       "      <td>beauti and the beast get paid homag by j k row...</td>\n",
       "      <td>Beauty and the Beast get paid Homage by J K Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spoilers now that we know that nagini was in f...</td>\n",
       "      <td>spoiler now that we know that nagini wa in fac...</td>\n",
       "      <td>spoiler now that we know that nagini wa in fac...</td>\n",
       "      <td>[spoilers] Now that we know that nagini was in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>merchandise monday welcome to merchandise mond...</td>\n",
       "      <td>merchandis monday welcom to merchandis monday ...</td>\n",
       "      <td>merchandis monday welcom to merchandis monday ...</td>\n",
       "      <td>Merchandise Monday! Welcome to Merchandise Mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how did witches save themselves when burning a...</td>\n",
       "      <td>how did witch save themselv when burn at the s...</td>\n",
       "      <td>how did witch save themselv when burn at the s...</td>\n",
       "      <td>How did witches save themselves when burning a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  \\\n",
       "0  they should make a movie about voldemort that ...   \n",
       "1  beauty and the beast get paid homage by j k ro...   \n",
       "2  spoilers now that we know that nagini was in f...   \n",
       "3  merchandise monday welcome to merchandise mond...   \n",
       "4  how did witches save themselves when burning a...   \n",
       "\n",
       "                                             post_st  \\\n",
       "0  they should make a movi about voldemort that e...   \n",
       "1  beauti and the beast get paid homag by j k row...   \n",
       "2  spoiler now that we know that nagini wa in fac...   \n",
       "3  merchandis monday welcom to merchandis monday ...   \n",
       "4  how did witch save themselv when burn at the s...   \n",
       "\n",
       "                                             post_lm  \\\n",
       "0  they should make a movi about voldemort that e...   \n",
       "1  beauti and the beast get paid homag by j k row...   \n",
       "2  spoiler now that we know that nagini wa in fac...   \n",
       "3  merchandis monday welcom to merchandis monday ...   \n",
       "4  how did witch save themselv when burn at the s...   \n",
       "\n",
       "                                            all_text  \n",
       "0  They should make a movie about Voldemort that ...  \n",
       "1  Beauty and the Beast get paid Homage by J K Ro...  \n",
       "2  [spoilers] Now that we know that nagini was in...  \n",
       "3  Merchandise Monday! Welcome to Merchandise Mon...  \n",
       "4  How did witches save themselves when burning a...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_resampled = pd.DataFrame(y_train_resampled, columns=['is_hp']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled_raw = X_train_resampled['post']\n",
    "X_test_raw = X_test['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('cls', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass the resampled training data to te model\n",
    "pipeline_resampled = Pipeline([\n",
    "    ('vect', cvt),\n",
    "    ('cls', MultinomialNB())\n",
    "]) \n",
    "pipeline_resampled.fit(X_train_resampled_raw, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}\n",
    "model_results['train_accuracy'] = pipeline_resampled.score(X_train_resampled_raw, y_train_resampled)\n",
    "model_results['test_accuracy'] = pipeline_resampled.score(X_test_raw, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_accuracy': 0.9731027857829011, 'test_accuracy': 0.9268887083671812}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling helped a little bit on the accuracy of training dataset (from 0.968 to 0.973), but doesn't really help further improve the accuracy on test set (from 0.933 to 0.927)...the original model is probably already doing good enough since the impact from imbalance is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV for fine tuning MultinomialNB base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('cv',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('mnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'cv': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'mnb': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'cv__analyzer': 'word',\n",
       " 'cv__binary': False,\n",
       " 'cv__decode_error': 'strict',\n",
       " 'cv__dtype': numpy.int64,\n",
       " 'cv__encoding': 'utf-8',\n",
       " 'cv__input': 'content',\n",
       " 'cv__lowercase': True,\n",
       " 'cv__max_df': 1.0,\n",
       " 'cv__max_features': None,\n",
       " 'cv__min_df': 1,\n",
       " 'cv__ngram_range': (1, 1),\n",
       " 'cv__preprocessor': None,\n",
       " 'cv__stop_words': 'english',\n",
       " 'cv__strip_accents': None,\n",
       " 'cv__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'cv__tokenizer': None,\n",
       " 'cv__vocabulary': None,\n",
       " 'mnb__alpha': 1.0,\n",
       " 'mnb__class_prior': None,\n",
       " 'mnb__fit_prior': True}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_steps = [('cv',cvt),\n",
    "            ('mnb',MultinomialNB())]\n",
    "\n",
    "# Fine tune parameters for MultinomialNB model\n",
    "mnb_params = {\"mnb__alpha\":np.arange(.05, 2, .05)}\n",
    "\n",
    "pipe = Pipeline(mnb_steps)\n",
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=N...nizer=None, vocabulary=None)), ('mnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'mnb__alpha': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  , 1.05, 1.1 ,\n",
       "       1.15, 1.2 , 1.25, 1.3 , 1.35, 1.4 , 1.45, 1.5 , 1.55, 1.6 , 1.65,\n",
       "       1.7 , 1.75, 1.8 , 1.85, 1.9 , 1.95])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GridSearchCV(pipe, mnb_params, cv=5) \n",
    "model.fit(X_train_raw, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_params': {'mnb__alpha': 0.9500000000000001},\n",
       " 'train_accuracy': 0.9693486590038314,\n",
       " 'test_accuracy': 0.9301380991064175,\n",
       " 'tn': 285,\n",
       " 'fp': 53,\n",
       " 'fn': 33,\n",
       " 'tp': 860,\n",
       " 'sensitivity/recall': 0.9630459126539753,\n",
       " 'specificity': 0.8431952662721893,\n",
       " 'precision': 0.9419496166484118}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_results = {}\n",
    "mnb_results['best_params'] = model.best_params_\n",
    "mnb_results['train_accuracy'] = model.score(X_train_raw, y_train)\n",
    "mnb_results['test_accuracy'] = model.score(X_test_raw, y_test)\n",
    "\n",
    "# Store confusion matrix results \n",
    "tn, fp, fn, tp = confusion_matrix(y_test, model.predict(X_test_raw)).ravel() \n",
    "mnb_results['tn'] = tn\n",
    "mnb_results['fp'] = fp\n",
    "mnb_results['fn'] = fn\n",
    "mnb_results['tp'] = tp\n",
    "mnb_results['sensitivity/recall'] = tp / (tp + fn)\n",
    "mnb_results['specificity'] = tn / (tn + fp)\n",
    "mnb_results['precision'] = tp / (tp + fp)\n",
    "\n",
    "\n",
    "mnb_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy improved a little bit from 0.929 to 0.930 with alpha changed from 1 to 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a sense of feature importance - what are the keywords in each subreddit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('cls', MultinomialNB(alpha=0.95, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', cvt),\n",
    "    ('cls', MultinomialNB(alpha=0.95))\n",
    "]) \n",
    "pipeline.fit(X_train_raw, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'cls': MultinomialNB(alpha=0.95, class_prior=None, fit_prior=True)}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = pipeline.named_steps['cls']\n",
    "cv = pipeline.named_steps['vect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers\n",
    "\n",
    "def important_features(vectorizer,classifier,n=30):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words for Fantastic Beasts subreddit\\n\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\\n\")\n",
    "    print(\"Important words for Harry Potter subreddit\\n\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words for Fantastic Beasts subreddit\n",
      "\n",
      "0 1037.0 grindelwald\n",
      "0 716.0 credence\n",
      "0 698.0 dumbledore\n",
      "0 579.0 movie\n",
      "0 567.0 newt\n",
      "0 554.0 just\n",
      "0 521.0 think\n",
      "0 502.0 know\n",
      "0 490.0 like\n",
      "0 315.0 leta\n",
      "0 297.0 beasts\n",
      "0 286.0 fantastic\n",
      "0 282.0 time\n",
      "0 276.0 maybe\n",
      "0 272.0 did\n",
      "0 266.0 albus\n",
      "0 238.0 don\n",
      "0 231.0 theory\n",
      "0 228.0 really\n",
      "0 223.0 love\n",
      "0 214.0 way\n",
      "0 212.0 people\n",
      "0 211.0 crimes\n",
      "0 209.0 queenie\n",
      "0 205.0 family\n",
      "0 198.0 harry\n",
      "0 198.0 does\n",
      "0 194.0 story\n",
      "0 189.0 tina\n",
      "0 187.0 didn\n",
      "-----------------------------------------\n",
      "\n",
      "Important words for Harry Potter subreddit\n",
      "\n",
      "1 2216.0 harry\n",
      "1 1095.0 just\n",
      "1 1027.0 like\n",
      "1 915.0 potter\n",
      "1 660.0 know\n",
      "1 654.0 think\n",
      "1 543.0 time\n",
      "1 524.0 voldemort\n",
      "1 513.0 books\n",
      "1 479.0 ve\n",
      "1 474.0 https\n",
      "1 468.0 hogwarts\n",
      "1 468.0 dumbledore\n",
      "1 459.0 book\n",
      "1 445.0 did\n",
      "1 441.0 don\n",
      "1 434.0 people\n",
      "1 415.0 com\n",
      "1 397.0 really\n",
      "1 390.0 house\n",
      "1 362.0 magic\n",
      "1 343.0 hufflepuff\n",
      "1 336.0 read\n",
      "1 332.0 world\n",
      "1 328.0 ravenclaw\n",
      "1 322.0 does\n",
      "1 311.0 slytherin\n",
      "1 310.0 hermione\n",
      "1 309.0 didn\n",
      "1 305.0 love\n"
     ]
    }
   ],
   "source": [
    "important_features(cv, mnb, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that the main characters / schools are the keywords that differentiate the subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a few iterations, the model that we found most accurate is the MultinomialNB on CountVectorizer with basic tokenizing (no stemming or lemmatizing).\n",
    "\n",
    "Steps are summarized in the README.me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use VotingClassifer in Ensemble to further optimize the model.\n",
    "\n",
    "The models could be further evaluated / optimized with new posts coming to the subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
